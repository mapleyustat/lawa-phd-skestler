==title:Roadmap{misc::roadmap}==

We are currently in the middle (or near to end) of the first stage.

==section:Stage: Everything you can do with BLAS you can do with FLENS/CXXBLAS==
==ul:begin==
-> Same performance
-> Less error prone (due to type safety, no pointer mess, expressive notation)
-> Possibility to easily extend the scope of BLAS in an orthogonal way:
--> for new matrix/vector types and
--> for relevant linear algebra operation for these new types
-> Generic implementation of BLAS routines
-> Extensions of BLAS routines:
--> Level 1 functions for matrices
    ({@@cxxblas::gecopy}, {@@cxxblas::geaxpy}, {@@cxxblas::gescal} and
    other functions that might be useful)
--> Level 3 functions, e.g. support 
    [$C \leftarrow \alpha A B^T$] where [$A$] is symmetric and [$B$] a general
    matrix.
-> Provide some sort of documentation.
==ul:end==

==subsection:Course of Action==
==ul:begin==
-> In a first run we limit the scope to matrix types with full storage schemes
   and dense vectors.
-> Hence:
--> We focus on {@@flens::DenseVector}, {@@flens::GeMatrix},
    {@@flens::HeMatrix}, {@@flens::SyMatrix}, {@@flens::TrMatrix}.
--> Once completed we also focus on matrix types with band storage schemes
    (these will be [GbMatrix], [HbMatrix], [SbMatrix], [TbMatrix])
--> Once completed we also focus on matrix types with packed storage schemes
    (these will be [HpMatrix], [SpMatrix], [TpMatrix])
==ul:end==

==section:Stage: Everything you can do with BLAS/LAPACK you can do with FLENS/CXXBLAS/CXXLAPACK==

==ul:begin==
-> Again: Same performance
-> Again: Less error prone (due to type safety, no pointer mess, expressive notation)
-> Again: Possibility to easily extend the scope of BLAS in an orthogonal way:
--> for new matrix/vector types and
--> relevant linear algebra operation for these new types
-> Again: Generic implementation of LAPACK routines based on CXXBLAS
--> Where possible or available the generic LAPACK implementation utilize
    native BLAS
--> If no native LAPACK installation is available we have a working reference
    implementation of LAPACK (makes FLENS easy to install)
--> We have LAPACK for multi-precision arithmetic!
==ul:end==

==subsection:Course of Action==
==ul:begin==
-> In a first run we limit the scope to some of the most important LAPACK routines
--> LU factorization
--> Cholesky factorization
--> QR factorization
--> SVD
-> Again we start with LAPACK routines for full storage schemes, then for
   band storage, then for packed storage.
-> To be realistic: we will not be able or willing to implement generic versions
   of each and every LAPACK function. But it should be sufficient:
--> to demonstrate a clear strategy on how to add missing LAPACK routines
    (native routines or generic routines)
--> if users are really, really missing a LAPACK routine they will add them
    or we will do so.
==ul:end==

==section:Stage: Add some experimental Sparse Matrices/Vectors==

==section:Performance==
==ul:begin==
-> In this stage we start tweaking the performance of the generic
   implementations in CXXBLAS/CXXLAPACK.
==ul:end==