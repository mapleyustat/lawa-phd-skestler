==title:Purpose==

==ul:begin==
-> Minimalistic C++ interface for BLAS
--> No container classes for matrices or vectors
--> Generic implementation of BLAS as an addition to native BLAS implementations
-> Low-level building block for other high-level C++ libraries
==ul:end==

==section:CXXBLAS compared to CBLAS/BLAS==

==ul:begin==
-> Function names in CBLAS/BLAS include the element types of the arguments.
--> [s] indicates [float]
--> [d] indicates [double]
--> [c] indicates complex numbers with single precision
--> [d] indicates complex numbers with double precision
-> For example:
--> [scopy] copies a vector of floats
--> [dcopy] copies a vector of doubles
--> [ccopy] copies a vector of complex numbers with single precision
--> [zcopy] copies a vector of complex numbers with double precision
-> Therefore using CBLAS from within C++ is tedious
--> You somehow have to manually map types to function names
-> CXXBLAS overloads functions for different element types
--> BLAS functions like [scopy], [dcopy], [ccopy], [zcopy] become simply [copy]
--> BLAS functions like [saxpy], [daxpy], [caxpy], [zaxpy] become simply [axpy]
--> ...
-> CXXBLAS is based on CBLAS (bringing its own [cblas.h] header which we
   derived from ATLAS's header).
==ul:end==

==section:Generic Implementation==
==ul:begin==
-> For few BLAS functions CXXBLAS provides a generic implementation.
--> Serves as a default/reference implementation if no native BLAS
    implementation is available on your system.
--> Allows using non-built-in types (e.g. types provided by the
    <a href="http://gmplib.org/">GMP library</a>) in linear algebra operations.
==ul:end==


==section: Why BLAS?==
==ul:begin==
-> The advantage of BLAS: Stable interface! For decades!
--> During the last two decades the C++ guys spent their time on re-inventing
    wheels.
--> No C++ library could achieve a state of stability matching the state of
    BLAS.  No C++ library got even close to this point.
--> At the same time the BLAS guys spent their time on tuning their
    implementations.  They achieve peak performance on the latest and greatest
    architectures.
--> Whenever the C++ guys figure out a special kind of operation that
    outperforms (or at least can compete with) a certain BLAS function
    it gets celebrated like a trip to mars.
--> In the BLAS world such things not even get noticed.
-> The advantage of BLAS: It provides exactly what is needed!
--> For many numerical algorithms you can look at the relevant linear algebra
    operations and (surprise, surprise) it turns out that you can perform them
    with a handful of BLAS operations.  
--> That means, if you can rely on a efficient BLAS implementation (and you can)
    then you can rely on an efficient implementation of the whole algorithm.
-> Conclusion: First step, assimilate the status quo. Second step, do better.
--> Primary goal: We need the possibility to take full advantage of BLAS
---> The capability to fully utilize BLAS is a necessary condition for any
     numerical library.
---> If you ignore BLAS you get ignored (by the people who count in this
     domain).
---> It is a necessary condition because: if you possibly want to be better in
     the future, then you should at least be as good as the status quo.
--> Secondary goal: We want a generic reference implementation of BLAS
==ul:end==
